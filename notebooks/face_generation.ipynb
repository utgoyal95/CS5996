{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2430225-5d7c-4d86-b6b6-19aac07b83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686fb4eb-93a0-4f69-9fbe-b61f3e630881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan_reproduction/train.py\n",
    "import random, pathlib, datetime\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216fbec0-052d-4b76-813d-ab9b5cdc626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Generator import Generator\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4ca88-50c0-4214-b0e4-20c7e15e4d51",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5218ea-e164-4a0c-838a-1b92ee09a411",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda2f1e7-9974-41a5-83ea-c5773e338721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 1337\n",
    "random.seed(SEED); torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "data = 'CelebA'\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH    = 128\n",
    "EPOCHS   = 25\n",
    "NZ       = 100\n",
    "LR       = 2e-4\n",
    "BETA1    = 0.5\n",
    "LABEL_FRAC = 0.10\n",
    "\n",
    "ROOT     = pathlib.Path(\"./\")\n",
    "DATA_DIR = ROOT/\"data\"\n",
    "RES_DIR  = ROOT/f\"results/{data}\"; RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR  = ROOT/f\"results/{data}/model\"; MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed1662-7959-48f6-970e-06240fae6f01",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fff52b-94e2-4ce5-8715-15c29ab48d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) TRANSFORMS for CelebA (RGB)\n",
    "transform = T.Compose([T.Resize(64), T.CenterCrop(64), T.ToTensor(),\n",
    "    # Normalize each channel to [-1,1], matching your tanh() output\n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ])\n",
    "\n",
    "# 2) DATALOADERS with explicit splits\n",
    "train_all = dset.CelebA(root=DATA_DIR, split=\"train\", \n",
    "                        download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_all, BATCH, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = dset.CelebA(root=DATA_DIR, split=\"test\", \n",
    "                       download=True, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(test_set, BATCH, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a55c1-4f82-4d49-ae0a-eeca69fc88c9",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4e4b7-47e5-4a15-8bfb-6e4dc6ea5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(nz=NZ, ngf=64, nc=3).to(DEVICE)\n",
    "netD = Discriminator(ndf=64, nc=3).to(DEVICE)\n",
    "\n",
    "netG.apply(lambda m: nn.init.normal_(m.weight.data, 0.0, 0.02) if hasattr(m,'weight') and m.weight is not None else None)\n",
    "netD.apply(lambda m: nn.init.normal_(m.weight.data, 0.0, 0.02) if hasattr(m,'weight') and m.weight is not None else None)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optD = optim.AdamW(netD.parameters(), lr=LR, betas=(BETA1,0.999), weight_decay=1e-4)\n",
    "optG = optim.AdamW(netG.parameters(), lr=LR, betas=(BETA1,0.999), weight_decay=1e-4)\n",
    "\n",
    "fixed_noise = torch.randn(64, NZ, 1, 1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38085249-fc01-415a-9d61-2eb80b3bb4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ─── Training ────────────────────────────────────────────────────────────────\n",
    "print(\"===> Training\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for i,(real,_ ) in enumerate(train_loader):\n",
    "        # Train D\n",
    "        netD.zero_grad()\n",
    "        real = real.to(DEVICE); b=real.size(0)\n",
    "        label_real = torch.ones(b, device=DEVICE)\n",
    "        loss_real = criterion(netD(real).view(-1), label_real)\n",
    "\n",
    "        noise = torch.randn(b, NZ,1,1, device=DEVICE)\n",
    "        fake = netG(noise)\n",
    "        label_fake = torch.zeros(b, device=DEVICE)\n",
    "        loss_fake = criterion(netD(fake.detach()).view(-1), label_fake)\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward(); optD.step()\n",
    "\n",
    "        # Train G\n",
    "        netG.zero_grad()\n",
    "        loss_G = criterion(netD(fake).view(-1), label_real)  # want ones\n",
    "        loss_G.backward(); optG.step()\n",
    "\n",
    "        if i%100==0:\n",
    "            print(f\"[E{epoch}/{EPOCHS}] [{i}/{len(train_loader)}] D:{loss_D.item():.3f} G:{loss_G.item():.3f}\")\n",
    "    # save epoch samples\n",
    "    with torch.no_grad():\n",
    "        vutils.save_image(netG(fixed_noise).cpu(), RES_DIR/f\"fake_epoch_{epoch:03d}.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6283f-0132-48f2-9a7b-ee18e7380afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===> Saving checkpoints\")\n",
    "torch.save(netG.state_dict(), MODEL_DIR/\"netG_final.pth\")\n",
    "torch.save(netD.state_dict(), MODEL_DIR/\"netD_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adce33-134e-46c5-929a-aa7b5e43870d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c472a-6bc6-4ef9-9915-8fea7fce65c1",
   "metadata": {},
   "source": [
    "## Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfeeaae-9847-4b99-951b-33cdc3685d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===> Linear‑probe evaluation\")\n",
    "\n",
    "lab_n = int(len(train_all)*LABEL_FRAC)\n",
    "lab_idx = np.random.choice(len(train_all), lab_n, False)\n",
    "train_lab_loader = DataLoader(Subset(train_all, lab_idx), BATCH, shuffle=False)\n",
    "POOL = nn.AdaptiveMaxPool2d((4,4))\n",
    "\n",
    "@torch.no_grad()\n",
    "def feats(loader):\n",
    "    out, ys = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        h = x\n",
    "        acts = []\n",
    "        for layer in netD.main:\n",
    "            h = layer(h)\n",
    "            if isinstance(layer, nn.Conv2d) and h.shape[2] >= 4 and h.shape[3] >= 4:\n",
    "                acts.append(POOL(h).cpu())\n",
    "        pooled = torch.cat([a.view(a.size(0), -1) for a in acts], dim=1)\n",
    "        out.append(pooled)\n",
    "        ys.append(y)\n",
    "    return torch.cat(out).numpy(), torch.cat(ys).numpy()\n",
    "\n",
    "netD.eval()\n",
    "\n",
    "X_tr, y_tr = feats(train_lab_loader)\n",
    "X_te, y_te = feats(test_loader)\n",
    "\n",
    "# 1) Standardize each feature across the training set\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1).fit(X_tr,y_tr)\n",
    "acc = accuracy_score(y_te, clf.predict(X_te))*100\n",
    "rep = f\"{datetime.datetime.now()}: linear‑probe ({LABEL_FRAC:.2%} labels) = {acc:.2f}%\\n\"\n",
    "\n",
    "print(rep); (RES_DIR/\"eval.txt\").write_text(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89095421-1a3b-419f-9723-41818de21b29",
   "metadata": {},
   "source": [
    "## Qualitative sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818e109-5bac-4164-947b-dd34edf65fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===> Qualitative sanity checks\")\n",
    "\n",
    "def latent_interpolation(n_steps=10):\n",
    "    z0,z1 = torch.randn(1,NZ,1,1,device=DEVICE), torch.randn(1,NZ,1,1,device=DEVICE)\n",
    "    alphas = torch.linspace(0,1,n_steps,device=DEVICE).view(-1,1,1,1)\n",
    "    z = z0 + alphas*(z1-z0)\n",
    "    with torch.no_grad(): imgs = netG(z).cpu()\n",
    "    vutils.save_image(imgs, RES_DIR/\"interpolation.png\", nrow=n_steps, normalize=True)\n",
    "\n",
    "latent_interpolation()\n",
    "\n",
    "# nearest‑neighbour check\n",
    "print(\"  computing nearest neighbours… (may take a minute)\")\n",
    "@torch.no_grad()\n",
    "def nn_check(num_fake=8, real_subset=6000):\n",
    "    # sample fakes\n",
    "    fake = netG(torch.randn(num_fake,NZ,1,1,device=DEVICE)).cpu()\n",
    "    # prep subset of real images for speed\n",
    "    sub_idx = np.random.choice(len(train_all), real_subset, False)\n",
    "    real_subset_imgs = torch.stack([train_all[i][0] for i in sub_idx])  # [R,1,64,64]\n",
    "    f = fake.view(num_fake,-1)\n",
    "    r = real_subset_imgs.view(real_subset,-1)\n",
    "    # compute L2 distances in chunks to save RAM\n",
    "    nn_real = []\n",
    "    for v in f:\n",
    "        dists = ((r - v).pow(2)).sum(1)\n",
    "        idx = torch.argmin(dists).item()\n",
    "        nn_real.append(real_subset_imgs[idx])\n",
    "    grid = torch.cat([fake, torch.stack(nn_real)],0)\n",
    "    vutils.save_image(grid, RES_DIR/\"nearest_neighbour.png\", nrow=num_fake, normalize=True)\n",
    "\n",
    "nn_check()\n",
    "\n",
    "print(\"Finished. See result images & logs in\", RES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ae061-6c8d-43fa-aa5f-6a7488b665f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
